{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation Related to Emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the iPython Notebook, it is important to install all necessary packages. To do that, in terminal type the command <code>pip install -r requirements.txt</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deniskrylov/Developer/PosEmotion/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from tools.detector import detect_poses\n",
    "from tools.extractor import Extractor\n",
    "from tools.metrics import label_probabilities\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before operating with data, it's important to see how the data looks like. For that purpose, let's convert <code>annotation.csv</code> file to pandas Dataframe. As we can see below, the Dataframe has the following structure:\n",
    "\n",
    "- Video Tag → The video identification present in YouTube. Use it to retrieve the source video. \n",
    "In this version of the dataset, the videos are present in the \"/Videos\" folder.\n",
    "- Clip Id → Id for each clip from a source video. This identification is unique within a source video. \n",
    "For a certain “Video Tag” with an “Clip Id”, the “Person Id” will be unique to a certain person. \n",
    "- Labels → An arrays of arrays containing the labels given by each annotator of the dataset.\n",
    "- Frame Number → The frame that was used for that annotation\n",
    "- X → Starting position of the bounding box in the x-axis\n",
    "- Y → Starting position of the bounding box in the y-axis\n",
    "- Width → % of the width of the video used as offset for “X”\n",
    "- Height → % of the height of the video used as offset for “Y”\n",
    "- Person Id → Integer to identify a certain person for clips with the same “Video Tag” and “Clip Id”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Tag</th>\n",
       "      <th>Clip Id</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Frame Number</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Person Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Happy'], ['Happy'], ['Happy']]</td>\n",
       "      <td>19532</td>\n",
       "      <td>41.965200</td>\n",
       "      <td>4.873195</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Happy'], ['Happy'], ['Happy']]</td>\n",
       "      <td>19538</td>\n",
       "      <td>41.564836</td>\n",
       "      <td>4.874640</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Happy'], ['Happy'], ['Happy']]</td>\n",
       "      <td>19544</td>\n",
       "      <td>41.164472</td>\n",
       "      <td>4.876086</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Happy'], ['Happy'], ['Happy']]</td>\n",
       "      <td>19550</td>\n",
       "      <td>40.764108</td>\n",
       "      <td>4.877532</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>[['Happy'], ['Happy'], ['Happy']]</td>\n",
       "      <td>19556</td>\n",
       "      <td>39.646728</td>\n",
       "      <td>5.014136</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Video Tag  Clip Id                             Labels  Frame Number  \\\n",
       "0  aJKL0ahn1Dk        1  [['Happy'], ['Happy'], ['Happy']]         19532   \n",
       "1  aJKL0ahn1Dk        1  [['Happy'], ['Happy'], ['Happy']]         19538   \n",
       "2  aJKL0ahn1Dk        1  [['Happy'], ['Happy'], ['Happy']]         19544   \n",
       "3  aJKL0ahn1Dk        1  [['Happy'], ['Happy'], ['Happy']]         19550   \n",
       "4  aJKL0ahn1Dk        1  [['Happy'], ['Happy'], ['Happy']]         19556   \n",
       "\n",
       "           X         Y      Width     Height  Person Id  \n",
       "0  41.965200  4.873195  44.216991  94.802684          0  \n",
       "1  41.564836  4.874640  44.216991  94.802684          0  \n",
       "2  41.164472  4.876086  44.216991  94.802684          0  \n",
       "3  40.764108  4.877532  44.216991  94.802684          0  \n",
       "4  39.646728  5.014136  44.216991  94.802684          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"assets/annotations/annotations.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each combination of <code>Video Tag</code>, <code>Clip Id</code> and <code>Person Id</code> represents a unique emotion related to a person. Therefore, we can split these emotions into segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 629\n",
      "First 5 segments: [(0, 27), (28, 39), (40, 51), (52, 69), (70, 77)]\n"
     ]
    }
   ],
   "source": [
    "extractor = Extractor(\n",
    "    \"assets/annotations/annotations.csv\",\n",
    "    \"assets/videos\",\n",
    "    \"assets/frames\"\n",
    ")\n",
    "\n",
    "# Uncomment the line below to extract frames from the videos\n",
    "# extractor.extract_frames()\n",
    "\n",
    "# Extracting the segments from the CSV file \n",
    "# (each segment represents a unique person in the fragment of video)\n",
    "segments = extractor.extract_segments()\n",
    "print(\"Number of segments:\", len(segments))\n",
    "print(\"First 5 segments:\", segments[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before pose detection, we need to convert our dataset in such a way, that the array of <code>Labels</code> column will be converted to multiple columns, where each column represents a probability of a particular emotion, calculated as $i/n$, where $i$ is an emotion label and $n$ is a total number of emotions that were detected by different annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Tag</th>\n",
       "      <th>Clip Id</th>\n",
       "      <th>Frame Number</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Person Id</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Anger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>19532</td>\n",
       "      <td>41.965200</td>\n",
       "      <td>4.873195</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>19538</td>\n",
       "      <td>41.564836</td>\n",
       "      <td>4.874640</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>19544</td>\n",
       "      <td>41.164472</td>\n",
       "      <td>4.876086</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>19550</td>\n",
       "      <td>40.764108</td>\n",
       "      <td>4.877532</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aJKL0ahn1Dk</td>\n",
       "      <td>1</td>\n",
       "      <td>19556</td>\n",
       "      <td>39.646728</td>\n",
       "      <td>5.014136</td>\n",
       "      <td>44.216991</td>\n",
       "      <td>94.802684</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Video Tag  Clip Id  Frame Number          X         Y      Width  \\\n",
       "0  aJKL0ahn1Dk        1         19532  41.965200  4.873195  44.216991   \n",
       "1  aJKL0ahn1Dk        1         19538  41.564836  4.874640  44.216991   \n",
       "2  aJKL0ahn1Dk        1         19544  41.164472  4.876086  44.216991   \n",
       "3  aJKL0ahn1Dk        1         19550  40.764108  4.877532  44.216991   \n",
       "4  aJKL0ahn1Dk        1         19556  39.646728  5.014136  44.216991   \n",
       "\n",
       "      Height  Person Id  Happy  Sad  Fear  Neutral  Surprise  Disgust  Anger  \n",
       "0  94.802684          0    1.0  0.0   0.0      0.0       0.0      0.0    0.0  \n",
       "1  94.802684          0    1.0  0.0   0.0      0.0       0.0      0.0    0.0  \n",
       "2  94.802684          0    1.0  0.0   0.0      0.0       0.0      0.0    0.0  \n",
       "3  94.802684          0    1.0  0.0   0.0      0.0       0.0      0.0    0.0  \n",
       "4  94.802684          0    1.0  0.0   0.0      0.0       0.0      0.0    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = label_probabilities(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract keypoints, different approaches will be used such as YOLO-Pose, DeepPose and OpenPose. For each of the approaches, a different dataframe will be created with coordinates of keypoints.\n",
    "\n",
    "- For each frame, a person will be detected (using ground truth) and cut out of the frame.\n",
    "- After for each frame pose detection algorithm will be applied.\n",
    "- At the end, csv file with keypoints will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YOLO-Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/deniskrylov/Developer/PosEmotion/assets/frames/Bqb2wT_eP_4_44059_1.jpg: 768x1280 3 persons, 1118.5ms\n",
      "Speed: 2.7ms preprocess, 1118.5ms inference, 244.6ms postprocess per image at shape (1, 3, 768, 1280)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: ultralytics.engine.results.Keypoints object\n",
      "masks: None\n",
      "names: {0: 'person'}\n",
      "obb: None\n",
      "orig_img: array([[[ 0,  1,  2],\n",
      "        [ 0,  1,  2],\n",
      "        [ 0,  2,  3],\n",
      "        ...,\n",
      "        [ 0,  0,  6],\n",
      "        [ 0,  0,  6],\n",
      "        [ 0,  0,  5]],\n",
      "\n",
      "       [[ 0,  0,  1],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  2],\n",
      "        ...,\n",
      "        [ 0,  0,  4],\n",
      "        [ 0,  0,  4],\n",
      "        [ 0,  0,  4]],\n",
      "\n",
      "       [[ 2,  4,  5],\n",
      "        [ 3,  5,  6],\n",
      "        [ 3,  5,  6],\n",
      "        ...,\n",
      "        [ 4,  7, 12],\n",
      "        [ 2,  5, 10],\n",
      "        [ 2,  5, 10]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[20, 28, 45],\n",
      "        [22, 30, 47],\n",
      "        [22, 30, 47],\n",
      "        ...,\n",
      "        [22, 31, 44],\n",
      "        [15, 24, 37],\n",
      "        [10, 19, 32]],\n",
      "\n",
      "       [[19, 29, 46],\n",
      "        [21, 31, 48],\n",
      "        [21, 31, 48],\n",
      "        ...,\n",
      "        [22, 31, 44],\n",
      "        [15, 24, 37],\n",
      "        [ 9, 18, 31]],\n",
      "\n",
      "       [[21, 31, 48],\n",
      "        [22, 32, 49],\n",
      "        [20, 30, 47],\n",
      "        ...,\n",
      "        [22, 31, 44],\n",
      "        [15, 24, 37],\n",
      "        [10, 19, 32]]], dtype=uint8)\n",
      "orig_shape: (720, 1280)\n",
      "path: '/Users/deniskrylov/Developer/PosEmotion/assets/frames/Bqb2wT_eP_4_44059_1.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/pose/predict'\n",
      "speed: {'preprocess': 2.719879150390625, 'inference': 1118.5178756713867, 'postprocess': 244.59195137023926}]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# result = detect_pose(\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#         \"/Users/deniskrylov/Developer/PosEmotion/assets/frames/{}_{}_{}.jpg\".format(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#         \"yolo\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_poses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/deniskrylov/Developer/PosEmotion/assets/frames/Bqb2wT_eP_4_44059_1.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# keypoints.append(result.to_dict())\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# result.draw_ipython()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(\"Progress: {}/{}\".format(progress+1, len(df)))\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# progress += 1\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/PosEmotion/tools/detector.py:34\u001b[0m, in \u001b[0;36mdetect_poses\u001b[0;34m(target, model, resize)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# for result in results:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#     for _, keypoint in enumerate(result.keypoints.xy.numpy().tolist()[0]):\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#         keypoints.append(keypoint)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChoose a valid model: yolo, openpose or alpha-pose!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mKeypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeypoints\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/PosEmotion/tools/keypoints.py:13\u001b[0m, in \u001b[0;36mKeypoints.__init__\u001b[0;34m(self, image, keys)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, keys):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage \u001b[38;5;241m=\u001b[39m image\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnose \u001b[38;5;241m=\u001b[39m \u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_eye \u001b[38;5;241m=\u001b[39m keys[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_eye \u001b[38;5;241m=\u001b[39m keys[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "keypoints = []\n",
    "progress = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # result = detect_pose(\n",
    "    #         \"/Users/deniskrylov/Developer/PosEmotion/assets/frames/{}_{}_{}.jpg\".format(\n",
    "    #             row[\"Video Tag\"], row[\"Frame Number\"], row[\"Person Id\"]\n",
    "    #         ),\n",
    "    #         \"yolo\"\n",
    "    #     )\n",
    "    result = detect_poses(\n",
    "        \"/Users/deniskrylov/Developer/PosEmotion/assets/frames/Bqb2wT_eP_4_44059_1.jpg\",\n",
    "        \"yolo\"\n",
    "    )\n",
    "    # keypoints.append(result.to_dict())\n",
    "    # result.draw_ipython()\n",
    "    # print(\"Progress: {}/{}\".format(progress+1, len(df)))\n",
    "    # progress += 1\n",
    "    break\n",
    "progress = 0\n",
    "\n",
    "keypoints_df = pd.DataFrame(keypoints)\n",
    "keypoints_df.to_csv(\"/Users/deniskrylov/Developer/PosEmotion/assets/annotations/yolo_keypoints.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DeepPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization has 2 parts: per image and per segment.\n",
    "\n",
    "- [WRONG] Per Image: all keypoints will be normalized according to the default size of the image $(w,h)$ and according to the size of a person on the image.\n",
    "- Per Segment: all segment sizes will be normalized to the default segment size $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame-wise Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Emotion Label Association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate Poses with Emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation and Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine Clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
